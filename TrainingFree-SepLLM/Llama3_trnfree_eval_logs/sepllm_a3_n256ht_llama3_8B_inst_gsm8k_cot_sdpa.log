2025-06-19:17:08:00,608 INFO     [__main__.py:279] Verbosity set to INFO
2025-06-19:17:08:00,660 INFO     [__init__.py:491] `group` and `group_alias` keys in TaskConfigs are deprecated and will be removed in v0.4.5 of lm_eval. The new `tag` field will be used to allow for a shortcut to a group of tasks one does not wish to aggregate metrics across. `group`s which aggregate across subtasks must be only defined in a separate group config file, which will be the official way to create groups that support cross-task aggregation as in `mmlu`. Please see the v0.4.4 patch notes and our documentation: https://github.com/EleutherAI/lm-evaluation-harness/blob/main/docs/new_task_guide.md#advanced-group-configs for more information.
2025-06-19:17:08:07,380 INFO     [__main__.py:376] Selected Tasks: ['gsm8k_cot']
2025-06-19:17:08:07,384 INFO     [evaluator.py:161] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234
2025-06-19:17:08:07,384 INFO     [evaluator.py:198] Initializing hf model, with arguments: {'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct', 'attn_implementation': 'sdpa', 'sepllm_config': './Llama3_trnfree_sepllm_configs/llama3_sepllm_a3_n256ht.yml'}
2025-06-19:17:08:08,552 INFO     [huggingface.py:130] Using device 'cuda:0'
/home/txiao/miniconda3/envs/py39_cu121_torch251_new2/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
2025-06-19:17:08:09,152 INFO     [huggingface.py:366] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:0'}
Warnings:>>> `self.Layer_num: int` (32), i.e., the number of layers of the current LM, must be correctly set!
############################################ Basic Args for SepAttention ############################################
self.USE_PREFILL_LOCAL_WIN_SIZES_wrt_LAYERS:  True
self.prefill_loc_win_size_list: [8192, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 8192]
self.USE_GENERATE_LOCAL_WIN_SIZES_wrt_LAYERS:  True
self.generate_win_loc_size_list: [8192, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 256, 8192]
self.PADDING_ID: 128009 --- Must be correctly set.
self.Layer_num: 32 --- Must be correctly set.
self.init_tok_max_idx: 2
self.USE_ORIGINAL_FULL_ATTEN:  False
self.streamingLLM:  False
self.BATCH_ADAPTIVE_INIT_POS:  True
>>> For `BATCH_ADAPTIVE_INIT_POS`: Typically True when the input_ids of the model use 'left padding', False for 'right padding'.
self.PRINT_KV_RATIO:  True
self.print_ratio_intervals:  10
>>>Note: Please be careful of the `separator_token_ids`, and make sure they are correct for the current LLM
self.separator_token_ids: [13, 11, 30, 0, 26, 25, 198, 220, 128000]
self.USE_BiPE: False
self.EXCLUDE_DIAGONAL:  True
>>>>>>>>---------##########################################################-----------<<<<<<<<
>>>>>>>>---------                                                          -----------<<<<<<<<
>>>>>>>>--------------------- Running our SepLLM strategy ----------------------------<<<<<<<<
>>>>>>>>---------                                                          -----------<<<<<<<<
>>>>>>>>---------##########################################################-----------<<<<<<<<
################>>>>>>>>>>> Current `config._attn_implementation`: sdpa <<<<<<<<<###################
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.64s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.31s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.11s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.30it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.04it/s]
2025-06-19:17:08:17,140 INFO     [evaluator.py:279] Setting fewshot random generator seed to 1234
2025-06-19:17:08:17,140 WARNING  [model.py:422] model.chat_template was called with the chat_template set to False or None. Therefore no chat template will be applied. Make sure this is an intended behavior.
2025-06-19:17:08:17,141 INFO     [task.py:423] Building contexts for gsm8k_cot on rank 0...
  0%|          | 0/1319 [00:00<?, ?it/s]  1%|          | 13/1319 [00:00<00:10, 128.21it/s]  2%|▏         | 26/1319 [00:00<00:10, 128.61it/s]  3%|▎         | 39/1319 [00:00<00:09, 129.08it/s]  4%|▍         | 53/1319 [00:00<00:09, 129.64it/s]  5%|▌         | 67/1319 [00:00<00:09, 130.63it/s]  6%|▌         | 81/1319 [00:00<00:09, 130.79it/s]  7%|▋         | 95/1319 [00:00<00:09, 131.09it/s]  8%|▊         | 109/1319 [00:00<00:09, 131.25it/s]  9%|▉         | 123/1319 [00:00<00:09, 131.52it/s] 10%|█         | 137/1319 [00:01<00:09, 131.18it/s] 11%|█▏        | 151/1319 [00:01<00:08, 131.26it/s] 13%|█▎        | 165/1319 [00:01<00:08, 131.88it/s] 14%|█▎        | 179/1319 [00:01<00:08, 132.45it/s] 15%|█▍        | 193/1319 [00:01<00:08, 132.30it/s] 16%|█▌        | 207/1319 [00:01<00:08, 132.36it/s] 17%|█▋        | 221/1319 [00:01<00:08, 132.20it/s] 18%|█▊        | 235/1319 [00:01<00:08, 132.20it/s] 19%|█▉        | 249/1319 [00:01<00:08, 132.34it/s] 20%|█▉        | 263/1319 [00:02<00:08, 129.51it/s] 21%|██        | 276/1319 [00:02<00:08, 129.12it/s] 22%|██▏       | 289/1319 [00:02<00:08, 127.76it/s] 23%|██▎       | 302/1319 [00:02<00:08, 126.13it/s] 24%|██▍       | 315/1319 [00:02<00:07, 125.83it/s] 25%|██▍       | 328/1319 [00:02<00:07, 126.98it/s] 26%|██▌       | 341/1319 [00:02<00:07, 127.22it/s] 27%|██▋       | 354/1319 [00:02<00:07, 127.73it/s] 28%|██▊       | 367/1319 [00:02<00:07, 128.19it/s] 29%|██▉       | 380/1319 [00:02<00:07, 128.58it/s] 30%|██▉       | 394/1319 [00:03<00:07, 130.37it/s] 31%|███       | 408/1319 [00:03<00:06, 131.57it/s] 32%|███▏      | 422/1319 [00:03<00:06, 132.55it/s] 33%|███▎      | 436/1319 [00:03<00:06, 132.93it/s] 34%|███▍      | 450/1319 [00:03<00:06, 132.29it/s] 35%|███▌      | 464/1319 [00:03<00:06, 132.43it/s] 36%|███▌      | 478/1319 [00:03<00:06, 132.74it/s] 37%|███▋      | 492/1319 [00:03<00:06, 133.71it/s] 38%|███▊      | 506/1319 [00:03<00:06, 134.36it/s] 39%|███▉      | 520/1319 [00:03<00:05, 134.10it/s] 40%|████      | 534/1319 [00:04<00:05, 133.98it/s] 42%|████▏     | 548/1319 [00:04<00:05, 133.02it/s] 43%|████▎     | 562/1319 [00:04<00:05, 133.52it/s] 44%|████▎     | 576/1319 [00:04<00:05, 133.78it/s] 45%|████▍     | 590/1319 [00:04<00:05, 134.06it/s] 46%|████▌     | 604/1319 [00:04<00:05, 134.22it/s] 47%|████▋     | 618/1319 [00:04<00:05, 133.77it/s] 48%|████▊     | 632/1319 [00:04<00:05, 132.48it/s] 49%|████▉     | 646/1319 [00:04<00:05, 131.99it/s] 50%|█████     | 660/1319 [00:05<00:04, 132.40it/s] 51%|█████     | 674/1319 [00:05<00:04, 132.94it/s] 52%|█████▏    | 688/1319 [00:05<00:04, 133.55it/s] 53%|█████▎    | 702/1319 [00:05<00:04, 133.05it/s] 54%|█████▍    | 716/1319 [00:05<00:04, 131.08it/s] 55%|█████▌    | 730/1319 [00:05<00:04, 129.24it/s] 56%|█████▋    | 743/1319 [00:05<00:04, 129.41it/s] 57%|█████▋    | 756/1319 [00:05<00:04, 129.16it/s] 58%|█████▊    | 769/1319 [00:05<00:04, 128.51it/s] 59%|█████▉    | 783/1319 [00:05<00:04, 129.95it/s] 60%|██████    | 797/1319 [00:06<00:03, 130.88it/s] 61%|██████▏   | 811/1319 [00:06<00:03, 131.39it/s] 63%|██████▎   | 825/1319 [00:06<00:03, 131.61it/s] 64%|██████▎   | 839/1319 [00:06<00:03, 131.20it/s] 65%|██████▍   | 853/1319 [00:06<00:03, 130.64it/s] 66%|██████▌   | 867/1319 [00:06<00:03, 130.83it/s] 67%|██████▋   | 881/1319 [00:06<00:03, 131.46it/s] 68%|██████▊   | 895/1319 [00:06<00:03, 131.94it/s] 69%|██████▉   | 909/1319 [00:06<00:03, 132.76it/s] 70%|██████▉   | 923/1319 [00:07<00:02, 133.31it/s] 71%|███████   | 937/1319 [00:07<00:02, 133.36it/s] 72%|███████▏  | 951/1319 [00:07<00:02, 132.84it/s] 73%|███████▎  | 965/1319 [00:07<00:02, 131.84it/s] 74%|███████▍  | 979/1319 [00:07<00:02, 130.90it/s] 75%|███████▌  | 993/1319 [00:07<00:02, 130.33it/s] 76%|███████▋  | 1007/1319 [00:07<00:02, 130.09it/s] 77%|███████▋  | 1021/1319 [00:07<00:02, 129.98it/s] 78%|███████▊  | 1034/1319 [00:07<00:02, 129.83it/s] 79%|███████▉  | 1048/1319 [00:07<00:02, 130.97it/s] 81%|████████  | 1062/1319 [00:08<00:01, 131.28it/s] 82%|████████▏ | 1076/1319 [00:08<00:01, 130.54it/s] 83%|████████▎ | 1090/1319 [00:08<00:01, 129.29it/s] 84%|████████▎ | 1103/1319 [00:08<00:01, 128.73it/s] 85%|████████▍ | 1117/1319 [00:08<00:01, 129.62it/s] 86%|████████▌ | 1131/1319 [00:08<00:01, 130.32it/s] 87%|████████▋ | 1145/1319 [00:08<00:01, 130.64it/s] 88%|████████▊ | 1159/1319 [00:08<00:01, 130.39it/s] 89%|████████▉ | 1173/1319 [00:08<00:01, 130.50it/s] 90%|████████▉ | 1187/1319 [00:09<00:01, 130.31it/s] 91%|█████████ | 1201/1319 [00:09<00:00, 130.27it/s] 92%|█████████▏| 1215/1319 [00:09<00:00, 129.99it/s] 93%|█████████▎| 1229/1319 [00:09<00:00, 130.16it/s] 94%|█████████▍| 1243/1319 [00:09<00:00, 130.55it/s] 95%|█████████▌| 1257/1319 [00:09<00:00, 130.47it/s] 96%|█████████▋| 1271/1319 [00:09<00:00, 130.04it/s] 97%|█████████▋| 1285/1319 [00:09<00:00, 130.22it/s] 98%|█████████▊| 1299/1319 [00:09<00:00, 130.82it/s]100%|█████████▉| 1313/1319 [00:10<00:00, 130.94it/s]100%|██████████| 1319/1319 [00:10<00:00, 131.03it/s]
2025-06-19:17:08:27,224 INFO     [evaluator.py:465] Running generate_until requests
Running generate_until requests:   0%|          | 0/1319 [00:00<?, ?it/s]/home/txiao/miniconda3/envs/py39_cu121_torch251_new2/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/txiao/miniconda3/envs/py39_cu121_torch251_new2/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Running generate_until requests:   0%|          | 1/1319 [00:29<10:45:45, 29.40s/it]Running generate_until requests:   5%|▌         | 71/1319 [00:54<13:22,  1.56it/s]  Running generate_until requests:  11%|█         | 141/1319 [01:18<09:13,  2.13it/s]Running generate_until requests:  16%|█▌        | 211/1319 [01:43<07:39,  2.41it/s]Running generate_until requests:  21%|██▏       | 281/1319 [02:07<06:42,  2.58it/s]Running generate_until requests:  27%|██▋       | 351/1319 [02:31<06:01,  2.68it/s]Running generate_until requests:  32%|███▏      | 421/1319 [02:55<05:26,  2.75it/s]Running generate_until requests:  37%|███▋      | 491/1319 [03:13<04:32,  3.04it/s]Running generate_until requests:  43%|████▎     | 561/1319 [03:30<03:47,  3.33it/s]
###############################SepAttention: Kept/Total tokens for this input batch#####################################
 (kept, total) : (971580, 2002560), ratio: 0.4852 

###############################SepAttention: Kept/Total tokens for all the inputs#######################################
 (kept, total) : (9122790, 19810560), ratio: 0.4605 
Running generate_until requests:  48%|████▊     | 631/1319 [03:54<03:35,  3.19it/s]Running generate_until requests:  53%|█████▎    | 701/1319 [04:16<03:13,  3.20it/s]Running generate_until requests:  58%|█████▊    | 771/1319 [04:31<02:36,  3.50it/s]Running generate_until requests:  64%|██████▍   | 841/1319 [04:47<02:08,  3.73it/s]Running generate_until requests:  69%|██████▉   | 911/1319 [05:11<01:58,  3.45it/s]Running generate_until requests:  74%|███████▍  | 981/1319 [05:24<01:26,  3.89it/s]Running generate_until requests:  80%|███████▉  | 1051/1319 [05:44<01:11,  3.72it/s]Running generate_until requests:  85%|████████▍ | 1121/1319 [06:08<00:57,  3.46it/s]Running generate_until requests:  90%|█████████ | 1191/1319 [06:19<00:31,  4.03it/s]Running generate_until requests:  96%|█████████▌| 1261/1319 [06:32<00:13,  4.37it/s]Running generate_until requests: 100%|██████████| 1319/1319 [06:32<00:00,  3.36it/s]
fatal: not a git repository (or any parent up to mount point /lustre)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
2025-06-19:17:15:05,708 INFO     [evaluation_tracker.py:269] Output path not provided, skipping saving results aggregated

hf (pretrained=meta-llama/Meta-Llama-3-8B-Instruct,attn_implementation=sdpa,sepllm_config=./Llama3_trnfree_sepllm_configs/llama3_sepllm_a3_n256ht.yml), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 70
|  Tasks  |Version|     Filter     |n-shot|  Metric   |   |Value |   |Stderr|
|---------|------:|----------------|-----:|-----------|---|-----:|---|-----:|
|gsm8k_cot|      3|flexible-extract|     8|exact_match|↑  |0.7824|±  |0.0114|
|         |       |strict-match    |     8|exact_match|↑  |0.7771|±  |0.0115|

