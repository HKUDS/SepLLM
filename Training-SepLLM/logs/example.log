[2025-06-22 20:51:22,745] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
fatal: not a git repository (or any parent up to mount point /lustre)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
NeoXArgs.from_ymls() ['../../sample_configs/sepllm-160m-on-pythia-with-pile_deduped-n64H-NOkernel.yml']
INFO:root:NeoXArgs.calculate_derived() Total number of GPUs determined to be: 8
-------------------- arguments --------------------
  attention_config ................ ['global', 'global', 'global', 'global', 'global', 'global', 'global', 'global', 'global', 'global', 'global', 'global']updated
  batch_size ...................... 32..........................updated
  bias_gelu_fusion ................ True........................updated
  checkpoint_activations .......... True........................updated
  checkpoint_factor ............... 1000........................updated
  config_files .................... {'sepllm-160m-on-pythia-with-pile_deduped-n64H-NOkernel.yml': '{\n  # ##########Debug##########\n  #   "world_size": 1,\n  #   "num_gpus": 1,\n  # ########################\n\n  # parallelism settings\n  "pipe-parallel-size": 1,\n  "model-parallel-size": 1,\n\n  # model settings\n  "num-layers": 12,\n  "hidden-size": 768,\n  "num-attention-heads": 12,\n  "seq-length": 2048,\n  "max-position-embeddings": 2048,\n  "pos-emb": "rotary",\n  "rotary-pct": 0.25,\n  "no-weight-tying": true,\n  "gpt-j-residual": true,\n  "output-layer-parallelism": "column",\n  "attention-config": [[["global"], 12]], \n\n  "scaled_masked_softmax_fusion": true, # For SepLLM\n  "bias-gelu-fusion": true,\n\n  # init methods\n  "init_method": "small_init",\n  "output_layer_init_method": "wang_init",\n\n  "optimizer": {\n    "type": "Adam",\n    "params": {\n      "lr": 0.0006,\n      "betas": [0.9, 0.95],\n      "eps": 1.0e-8\n    }\n  },\n  "min_lr": 0.00006,\n\n  "zero_optimization": {\n    "stage": 1,\n    "allgather_partitions": true,\n    "allgather_bucket_size": 500000000,\n    "overlap_comm": true,\n    "reduce_scatter": true,\n    "reduce_bucket_size": 500000000,\n    "contiguous_gradients": true,\n    "cpu_offload": false\n  },\n\n  # batch size (trained on 8 gpus)\n  "train_micro_batch_size_per_gpu": 32, # For 8 cards: if your GPU memory is not enough, you can set 64x2, 32x4, 16x8, etc., for 8 cards\' machine, to keep the global train_batch_size as 1024.\n  "gradient_accumulation_steps": 4,  # For 8 cards: if your GPU memory is not enough, you can set 64x2, 32x4, 16x8, etc., for 8 cards\' machine, to keep the global train_batch_size as 1024.\n  "data-impl": "mmap",\n  "num_workers": 1,\n\n  # activation checkpointing\n  "checkpoint-activations": true,\n  "checkpoint-num-layers": 1,\n  "partition-activations": true,\n  "synchronize-each-layer": true,\n\n  # regularization\n  "gradient_clipping": 1.0,\n  "weight-decay": 0.1,\n  "hidden-dropout": 0,\n  "attention-dropout": 0,\n\n  # precision settings\n  "fp16": {\n    "fp16": true,\n    "enabled": true,\n    "loss_scale": 0,\n    "loss_scale_window": 1000,\n    "initial_scale_power": 12,\n    "hysteresis": 2,\n    "min_loss_scale": 1\n  },\n\n  "train-iters": 143000,\n  "lr-decay-iters": 143000,\n  "distributed-backend": "nccl",\n  "lr-decay-style": "cosine",\n  "warmup": 0.01,\n  "checkpoint-factor": 1000,\n  "extra-save-iters": [0,1,2,4,8,16,32,64,128,256,512],\n  "eval-interval": 4000, \n  "eval-iters": 10,\n\n  "log-interval": 10,\n  "steps_per_print": 10,\n  "wall_clock_breakdown": true,\n\n  "train-data-paths": ["/lustre/fast/fast/txiao/shihan/compressed_pythia_data/pile_0.87_deduped_text_document"],\n  "valid-data-paths": ["/lustre/fast/fast/txiao/shihan/compressed_pythia_data/pile_0.87_deduped_text_document"],\n  "test-data-paths": ["/lustre/fast/fast/txiao/shihan/compressed_pythia_data/pile_0.87_deduped_text_document"],\n\n  "tokenizer-type": "HFTokenizer",\n  "vocab-file": "/lustre/fast/fast/txiao/shihan_new/workspace/SepLLM/Training-SepLLM/sample_configs/20B_tokenizer.json",\n\n  "launcher": "pdsh",\n\n  # "save": "path/to/SepLLM-160m/checkpoints_n64h_8cards",\n  # "load": "path/to/SepLLM-160m/checkpoints_n64h_8cards",\n  \n  ####################################################################### SepLLM #######################################################################:\n  # "hostfile": "path/to/SepLLM/Training-SepLLM/sample_configs/hostfile",\n\n  \'separator_token_ids\': [15, 13, 32, 2, 28, 27, 209, 186, 187], # For Pythia tokenizer  ## Fixed; The token ids for the special tokens (i.e. separators):  [\'.\', \',\', \'?\', \'!\', \';\', ":", \' \', \'\\t\',\'\\n\'].\n  \'PADDING_ID\': 0 , # For Pythia tokenizer  ## Fixed; The id for padding token of Pythia (GPT_NeoX)\n  \n\n  \'USE_PREFILL_LOCAL_WIN_SIZES_wrt_LAYERS\'  :   True,  # If True: the prefilling local window sizes for different self-attention layers are different; If True: should set \'prefill_loc_win_size_list\', else: should set \'prefill_local_window_size\'\n  \'USE_GENERATE_LOCAL_WIN_SIZES_wrt_LAYERS\' :   True,  # If True: the generating local window sizes for different self-attention layers are different; If True: should set \'generate_win_loc_size_list\', else: should set \'generate_local_window_size\'. USE_GENERATE_LOCAL_WIN_SIZES_wrt_LAYERS does not have any effect during the pretraining/prefilling phase.    \n\n  \'prefill_loc_win_size_list\' : [2048,   64,   64,   64,   64,   64,   64,   64,   64,   64, \n                                 64,   64],            ## The local window sizes for different self-attention layers when training (or prefilling). KVs for tokens inside the local window (we call them \'Neighboring Tokens\') are kept and can been seen by the current token.                      \n\n  \'generate_win_loc_size_list\': [2048,   64,   64,   64,   64,   64,   64,   64,   64,   64, \n                                 64,   64],            ## The local window sizes for different self-attention layers when generating. KVs for tokens inside the local window (we call them \'Neighboring Tokens\') are kept and can been seen by the current token. generate_win_loc_size_list does not have any effect during the pretraining/prefilling phase.\n\n  \'init_tok_max_idx\' :  2, # The largest index for the kept initial tokens. E.g., if init_tok_max_idx==2, it means we keep 3 initial tokens (idx: 0,1,2)\n  ######################################There should be at most 1 True for the following 3 args ##############################################\n  \'USE_ORIGINAL_FULL_ATTEN\' : False,  # Flag signal with the highest priority.  Train the Pythia model without any modification (standard full-attention version, i.e., standard upper triangular mask) if True.\n  \'streamingLLM\' :  False,  # Train streamingLLM. Only takes effect when USE_ORIGINAL_FULL_ATTEN=False. \n  \'USE_SEP_ATTN_KERNEL_ACCELERATOR\': False, # If True, use Sep_Attention module\'s kernel accelerator to accelerate the training process of SepLLM. If False (together with USE_ORIGINAL_FULL_ATTEN=False and streamingLLM=False), run plain SepLLM\n  ######################################There should be at most 1 True for the above 3 args ##############################################\n\n  \'BATCH_ADAPTIVE_INIT_POS\' : False,  # False by default.  If True: use the floating initial tokens\' starting positions since when evaluating, LLM usually add paddings on the left side of the shorter seqs in a batch for alignment (i.e., left padding). Can be False when pretraining since the starting positions of initial tokens are at the beginning of each sequence in a batch for pretraining (i.e., right padding)\n  \'PRINT_KV_RATIO\' :  True,  # If True, print the KV cache preservation ratio (especially for the released trained model during generating). When pretraining, it will also print the retention ratio for the computational complexity of calculating the attention map if it is set True\n  \'print_ratio_intervals\': 8000,   # Print the retention ratio for the computational complexity of calculating the attention map once after every \'print_ratio_intervals\' forward passes (or print_ratio_intervals/gradient_accumulation_steps  iterations). It only takes effect when PRINT_KV_RATIO=True.    \n  ####################################################################### ### #######################################################################\n}\n'}updated
  data_impl ....................... mmap........................updated
  dynamic_loss_scale .............. True........................updated
  eval_interval ................... 4000........................updated
  eval_iters ...................... 10..........................updated
  extra_save_iters ................ [0, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512]updated
  fp16 ............................ {'fp16': True, 'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 12, 'hysteresis': 2, 'min_loss_scale': 1}updated
  generate_win_loc_size_list ...... [2048, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]updated
  global_num_gpus ................. 8...........................updated
  gpt_j_residual .................. True........................updated
  gradient_accumulation_steps ..... 4...........................updated
  hidden_size ..................... 768.........................updated
  init_method ..................... small_init..................updated
  is_pipe_parallel ................ True........................updated
  log_interval .................... 10..........................updated
  lr .............................. 0.0006......................updated
  lr_decay_iters .................. 143000......................updated
  lr_decay_style .................. cosine......................updated
  max_position_embeddings ......... 2048........................updated
  min_lr .......................... 6e-05.......................updated
  no_weight_tying ................. True........................updated
  num_attention_heads ............. 12..........................updated
  num_layers ...................... 12..........................updated
  num_workers ..................... 1...........................updated
  optimizer ....................... {'type': 'Adam', 'params': {'lr': 0.0006, 'betas': [0.9, 0.95], 'eps': 1e-08}}updated
  optimizer_type .................. Adam........................updated
  output_layer_init_method ........ wang_init...................updated
  partition_activations ........... True........................updated
  pipe_parallel_size .............. 1...........................updated
  pos_emb ......................... rotary......................updated
  precision ....................... fp16........................updated
  prefill_loc_win_size_list ....... [2048, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]updated
  PRINT_KV_RATIO .................. True........................updated
  rotary_pct ...................... 0.25........................updated
  save_iters ...................... [0, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000, 11000, 12000, 13000, 14000, 15000, 16000, 17000, 18000, 19000, 20000, 21000, 22000, 23000, 24000, 25000, 26000, 27000, 28000, 29000, 30000, 31000, 32000, 33000, 34000, 35000, 36000, 37000, 38000, 39000, 40000, 41000, 42000, 43000, 44000, 45000, 46000, 47000, 48000, 49000, 50000, 51000, 52000, 53000, 54000, 55000, 56000, 57000, 58000, 59000, 60000, 61000, 62000, 63000, 64000, 65000, 66000, 67000, 68000, 69000, 70000, 71000, 72000, 73000, 74000, 75000, 76000, 77000, 78000, 79000, 80000, 81000, 82000, 83000, 84000, 85000, 86000, 87000, 88000, 89000, 90000, 91000, 92000, 93000, 94000, 95000, 96000, 97000, 98000, 99000, 100000, 101000, 102000, 103000, 104000, 105000, 106000, 107000, 108000, 109000, 110000, 111000, 112000, 113000, 114000, 115000, 116000, 117000, 118000, 119000, 120000, 121000, 122000, 123000, 124000, 125000, 126000, 127000, 128000, 129000, 130000, 131000, 132000, 133000, 134000, 135000, 136000, 137000, 138000, 139000, 140000, 141000, 142000]updated
  scaled_masked_softmax_fusion .... True........................updated
  separator_token_ids ............. [15, 13, 32, 2, 28, 27, 209, 186, 187]updated
  seq_length ...................... 2048........................updated
  sparsity_config ................. {}..........................updated
  synchronize_each_layer .......... True........................updated
  test_data_paths ................. ['/lustre/fast/fast/txiao/shihan/compressed_pythia_data/pile_0.87_deduped_text_document']updated
  test_data_weights ............... [1.0].......................updated
  text_gen_type ................... unconditional...............updated
  tokenizer_type .................. HFTokenizer.................updated
  train_batch_size ................ 1024........................updated
  train_data_paths ................ ['/lustre/fast/fast/txiao/shihan/compressed_pythia_data/pile_0.87_deduped_text_document']updated
  train_data_weights .............. [1.0].......................updated
  train_iters ..................... 143000......................updated
  train_micro_batch_size_per_gpu .. 32..........................updated
  USE_GENERATE_LOCAL_WIN_SIZES_wrt_LAYERS  True.................updated
  USE_PREFILL_LOCAL_WIN_SIZES_wrt_LAYERS  True..................updated
  USE_SEP_ATTN_KERNEL_ACCELERATOR . False.......................updated
  user_script ..................... ../../train.py..............updated
  valid_data_paths ................ ['/lustre/fast/fast/txiao/shihan/compressed_pythia_data/pile_0.87_deduped_text_document']updated
  valid_data_weights .............. [1.0].......................updated
  vocab_file ...................... /lustre/fast/fast/txiao/shihan_new/workspace/SepLLM/Training-SepLLM/sample_configs/20B_tokenizer.jsonupdated
  wall_clock_breakdown ............ True........................updated
  zero_allgather_bucket_size ...... 500000000...................updated
  zero_contiguous_gradients ....... True........................updated
  zero_optimization ............... {'stage': 1, 'allgather_partitions': True, 'allgather_bucket_size': 500000000, 'overlap_comm': True, 'reduce_scatter': True, 'reduce_bucket_size': 500000000, 'contiguous_gradients': True, 'cpu_offload': False}updated
  zero_reduce_bucket_size ......... 500000000...................updated
  zero_reduce_scatter ............. True........................updated
  zero_stage ...................... 1...........................updated
  _DEFAULT_SPARSE_BLOCK_SIZE ...... 128.........................default
  account ......................... None........................default
  activation ...................... gelu........................default
  activation_checkpointing ........ None........................default
  adlr_autoresume ................. False.......................default
  adlr_autoresume_interval ........ 1000........................default
  allow_chopped ................... True........................default
  amp ............................. None........................default
  apply_query_key_layer_scaling ... False.......................default
  attention_dropout ............... 0...........................default
  attention_softmax_in_fp32 ....... False.......................default
  autotuning ...................... None........................default
  autotuning_run .................. None........................default
  base_shapes_file ................ None........................default
  BATCH_ADAPTIVE_INIT_POS ......... False.......................default
  bf16 ............................ None........................default
  bias_dropout_fusion ............. False.......................default
  BiPE_seps ....................... None........................default
  char_level_ppl .................. False.......................default
  checkpoint ...................... None........................default
  checkpoint_in_cpu ............... False.......................default
  checkpoint_num_layers ........... 1...........................default
  checkpoint_scale ................ linear......................default
  checkpoint_validation_with_forward_pass  False................default
  clip_grad ....................... 1.0.........................default
  comment ......................... None........................default
  comms_logger .................... None........................default
  communication_data_type ......... None........................default
  compression_training ............ None........................default
  contiguous_checkpointing ........ False.......................default
  coord_check ..................... False.......................default
  create_moe_param_group .......... True........................default
  csv_monitor ..................... None........................default
  curriculum_learning ............. None........................default
  curriculum_seqlen ............... 0...........................default
  data_efficiency ................. None........................default
  data_path ....................... None........................default
  data_types ...................... None........................default
  deepscale ....................... False.......................default
  deepscale_config ................ None........................default
  deepspeed ....................... True........................default
  deepspeed_activation_checkpointing  True......................default
  deepspeed_extra_args ............ None........................default
  deepspeed_mpi ................... False.......................default
  deepspeed_slurm ................. False.......................default
  detect_nvlink_pairs ............. False.......................default
  distributed_backend ............. nccl........................default
  do_test ......................... None........................default
  do_train ........................ None........................default
  do_valid ........................ None........................default
  dump_state ...................... False.......................default
  elasticity ...................... None........................default
  enable_expert_tensor_parallelism  False.......................default
  eod_mask_loss ................... False.......................default
  eval_results_prefix ............. ............................default
  eval_tasks ...................... None........................default
  exclude ......................... None........................default
  EXCLUDE_DIAGONAL ................ True........................default
  exit_interval ................... None........................default
  expert_interval ................. 2...........................default
  finetune ........................ False.......................default
  flops_profiler .................. None........................default
  force_multi ..................... False.......................default
  fp16_lm_cross_entropy ........... False.......................default
  fp32_allreduce .................. False.......................default
  generate_k ...................... 0...........................default
  generate_local_window_size ...... 256.........................default
  git_hash ........................ None........................default
  gmlp_attn_dim ................... 64..........................default
  gpt_j_tied ...................... False.......................default
  gradient_clipping ............... 1.0.........................default
  gradient_noise_scale_cpu_offload  False.......................default
  gradient_noise_scale_n_batches .. 5...........................default
  gradient_predivide_factor ....... 1.0.........................default
  hidden_dropout .................. 0...........................default
  hostfile ........................ None........................default
  hysteresis ...................... 2...........................default
  include ......................... None........................default
  init_method_std ................. 0.02........................default
  init_tok_max_idx ................ 2...........................default
  intermediate_size ............... None........................default
  iteration ....................... None........................default
  keep_last_n_checkpoints ......... None........................default
  launcher ........................ pdsh........................default
  layernorm_epsilon ............... 1e-05.......................default
  layernorm_fusion ................ False.......................default
  lazy_mpu_init ................... False.......................default
  load ............................ None........................default
  local_rank ...................... None........................default
  log_dir ......................... None........................default
  log_grad_norm ................... False.......................default
  log_grad_pct_zeros .............. False.......................default
  log_gradient_noise_scale ........ False.......................default
  log_optimizer_states ............ False.......................default
  log_param_norm .................. False.......................default
  loss_scale ...................... None........................default
  loss_scale_window ............... 1000.0......................default
  make_vocab_size_divisible_by .... 128.........................default
  mamba_causal_conv_fusion ........ False.......................default
  mamba_inner_func_fusion ......... False.......................default
  mamba_selective_fp32_params ..... True........................default
  mamba_selective_scan_fusion ..... False.......................default
  mamba_use_bias_in_conv .......... True........................default
  mamba_use_bias_in_linears ....... False.......................default
  master_addr ..................... None........................default
  master_port ..................... 29500.......................default
  maximum_tokens .................. 64..........................default
  memory_profiling ................ False.......................default
  memory_profiling_path ........... None........................default
  merge_file ...................... None........................default
  min_scale ....................... 1.0.........................default
  mlp_type ........................ regular.....................default
  mmap_warmup ..................... False.......................default
  model_parallel_size ............. 1...........................default
  moe_eval_capacity_factor ........ 1.0.........................default
  moe_expert_parallel_size ........ 1...........................default
  moe_glu ......................... False.......................default
  moe_jitter_eps .................. None........................default
  moe_lbl_in_fp32 ................. False.......................default
  moe_loss_coeff .................. 0.1.........................default
  moe_min_capacity ................ 4...........................default
  moe_num_experts ................. 1...........................default
  moe_token_dropping .............. False.......................default
  moe_top_k ....................... 1...........................default
  moe_train_capacity_factor ....... 1.0.........................default
  moe_type ........................ megablocks..................default
  moe_use_residual ................ True........................default
  mup_attn_temp ................... 1.0.........................default
  mup_embedding_mult .............. 1.0.........................default
  mup_init_scale .................. 1.0.........................default
  mup_output_temp ................. 1.0.........................default
  mup_rp_embedding_mult ........... 1.0.........................default
  mup_width_scale ................. 2...........................default
  no_load_optim ................... False.......................default
  no_load_rng ..................... False.......................default
  no_save_optim ................... False.......................default
  no_save_rng ..................... False.......................default
  no_ssh_check .................... False.......................default
  norm ............................ layernorm...................default
  num_gpus ........................ None........................default
  num_kv_heads .................... None........................default
  num_nodes ....................... -1..........................default
  num_samples ..................... 1...........................default
  num_unique_layers ............... None........................default
  onnx_safe ....................... False.......................default
  opt_pos_emb_offset .............. 0...........................default
  output_layer_parallelism ........ column......................default
  override_lr_scheduler ........... False.......................default
  pack_impl ....................... packed......................default
  padded_vocab_size ............... None........................default
  PADDING_ID ...................... 0...........................default
  param_sharing_style ............. grouped.....................default
  pipe_partition_method ........... type:transformer|mlp........default
  prefill_k ....................... 0...........................default
  prefill_local_window_size ....... 256.........................default
  prescale_gradients .............. False.......................default
  print_ratio_intervals ........... 8000........................default
  profile ......................... False.......................default
  profile_backward ................ False.......................default
  profile_step_start .............. 10..........................default
  profile_step_stop ............... 12..........................default
  prompt_end ...................... 
...........................default
  rank ............................ None........................default
  RECOMPILE_SEP_ATTN_KERNEL ....... False.......................default
  recompute ....................... False.......................default
  return_logits ................... False.......................default
  rms_norm_epsilon ................ 1e-08.......................default
  rope_fusion ..................... False.......................default
  rotary_emb_base ................. 10000.......................default
  rotary_save_freqs_buffer ........ False.......................default
  rpe_max_distance ................ 128.........................default
  rpe_num_buckets ................. 32..........................default
  s3_chunk_size ................... 104857600...................default
  s3_path ......................... None........................default
  SA_Denominator_Bias ............. 1e-10.......................default
  SA_Numerator_Bias ............... 0.0.........................default
  sample_input_file ............... None........................default
  sample_output_file .............. samples.txt.................default
  save ............................ None........................default
  save_base_shapes ................ False.......................default
  scaled_upper_triang_masked_softmax_fusion  False..............default
  scalenorm_epsilon ............... 1e-08.......................default
  scheduler ....................... None........................default
  seed ............................ 1234........................default
  sequence_parallel ............... False.......................default
  short_seq_prob .................. 0.1.........................default
  sliding_window_width ............ None........................default
  soft_prompt_tuning .............. None........................default
  sparse_attention ................ None........................default
  sparse_gradients ................ False.......................default
  split ........................... 969, 30, 1..................default
  steps_per_print ................. 10..........................default
  streamingLLM .................... False.......................default
  temperature ..................... 0.0.........................default
  tensorboard ..................... None........................default
  tensorboard_dir ................. None........................default
  test_label_data_paths ........... None........................default
  top_k ........................... 0...........................default
  top_p ........................... 0.0.........................default
  train_label_data_paths .......... None........................default
  use_bias_in_attn_linear ......... True........................default
  use_bias_in_norms ............... True........................default
  USE_BiPE ........................ False.......................default
  use_bnb_optimizer ............... False.......................default
  use_checkpoint_lr_scheduler ..... False.......................default
  use_cpu_initialization .......... False.......................default
  use_mup ......................... False.......................default
  USE_ORIGINAL_FULL_ATTEN ......... False.......................default
  use_qk_layernorm ................ False.......................default
  USE_SA_SOFTMAX .................. False.......................default
  USE_SA_SOFTMAX_NO_DENO .......... False.......................default
  use_shared_fs ................... True........................default
  use_tutel ....................... False.......................default
  use_wandb ....................... None........................default
  valid_label_data_paths .......... None........................default
  wandb ........................... None........................default
  wandb_group ..................... None........................default
  wandb_host ...................... https://api.wandb.ai........default
  wandb_init_all_ranks ............ False.......................default
  wandb_project ................... neox........................default
  wandb_team ...................... None........................default
  warmup .......................... 0.01........................default
  weight_by_num_documents ......... False.......................default
  weight_decay .................... 0.1.........................default
  weighted_sampler_alpha .......... 1.0.........................default
  world_size ...................... None........................default
---------------- end of arguments ----------------
>>>>>>>>---------##########################################################-----------<<<<<<<<
>>>>>>>>---------               Start Checking Arguments                   -----------<<<<<<<<
>>>>>>>>---------##########################################################-----------<<<<<<<<


Warnings: It is recommended to set the value of generate_win_loc_size_list=[2048, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64] to be the same as prefill_loc_win_size_list=[2048, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64], even though generate_win_loc_size_list does not have any effect during the pretraining/prefilling phase.
>>>>>>>>---------##########################################################-----------<<<<<<<<
>>>>>>>>---------                 Initial Check Finished                   -----------<<<<<<<<
>>>>>>>>---------##########################################################-----------<<<<<<<<


NeoXArgs.configure_distributed_args() using world size: 1 and model-parallel size: 1 
[2025-06-22 20:51:31,098] [WARNING] [runner.py:217:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-06-22 20:51:31,098] [INFO] [runner.py:586:main] cmd = /home/txiao/miniconda3/envs/py38_cu121_torch21_new/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None ../../train.py --deepspeed_config eyJ0cmFpbl9iYXRjaF9zaXplIjogMTAyNCwgInRyYWluX21pY3JvX2JhdGNoX3NpemVfcGVyX2dwdSI6IDMyLCAiZ3JhZGllbnRfYWNjdW11bGF0aW9uX3N0ZXBzIjogNCwgIm9wdGltaXplciI6IHsidHlwZSI6ICJBZGFtIiwgInBhcmFtcyI6IHsibHIiOiAwLjAwMDYsICJiZXRhcyI6IFswLjksIDAuOTVdLCAiZXBzIjogMWUtMDh9fSwgImZwMTYiOiB7ImZwMTYiOiB0cnVlLCAiZW5hYmxlZCI6IHRydWUsICJsb3NzX3NjYWxlIjogMCwgImxvc3Nfc2NhbGVfd2luZG93IjogMTAwMCwgImluaXRpYWxfc2NhbGVfcG93ZXIiOiAxMiwgImh5c3RlcmVzaXMiOiAyLCAibWluX2xvc3Nfc2NhbGUiOiAxfSwgInplcm9fb3B0aW1pemF0aW9uIjogeyJzdGFnZSI6IDEsICJhbGxnYXRoZXJfcGFydGl0aW9ucyI6IHRydWUsICJhbGxnYXRoZXJfYnVja2V0X3NpemUiOiA1MDAwMDAwMDAsICJvdmVybGFwX2NvbW0iOiB0cnVlLCAicmVkdWNlX3NjYXR0ZXIiOiB0cnVlLCAicmVkdWNlX2J1Y2tldF9zaXplIjogNTAwMDAwMDAwLCAiY29udGlndW91c19ncmFkaWVudHMiOiB0cnVlLCAiY3B1X29mZmxvYWQiOiBmYWxzZX0sICJ3YWxsX2Nsb2NrX2JyZWFrZG93biI6IHRydWV9 --megatron_config {"train_batch_size": 1024, "train_micro_batch_size_per_gpu": 32, "gradient_accumulation_steps": 4, "optimizer": {"type": "Adam", "params": {"lr": 0.0006, "betas": [0.9, 0.95], "eps": 1e-08}}, "fp16": {"fp16": true, "enabled": true, "loss_scale": 0, "loss_scale_window": 1000, "initial_scale_power": 12, "hysteresis": 2, "min_loss_scale": 1}, "zero_optimization": {"stage": 1, "allgather_partitions": true, "allgather_bucket_size": 500000000, "overlap_comm": true, "reduce_scatter": true, "reduce_bucket_size": 500000000, "contiguous_gradients": true, "cpu_offload": false}, "wall_clock_breakdown": true, "precision": "fp16", "num_layers": 12, "hidden_size": 768, "num_attention_heads": 12, "seq_length": 2048, "max_position_embeddings": 2048, "pos_emb": "rotary", "no_weight_tying": true, "attention_config": ["global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global", "global"], "sparsity_config": {}, "scaled_masked_softmax_fusion": true, "bias_gelu_fusion": true, "rotary_pct": 0.25, "init_method": "small_init", "output_layer_init_method": "wang_init", "gpt_j_residual": true, "lr_decay_style": "cosine", "lr_decay_iters": 143000, "min_lr": 6e-05, "optimizer_type": "Adam", "zero_stage": 1, "zero_reduce_scatter": true, "zero_contiguous_gradients": true, "zero_reduce_bucket_size": 500000000, "zero_allgather_bucket_size": 500000000, "lr": 0.0006, "tokenizer_type": "HFTokenizer", "train_data_paths": ["/lustre/fast/fast/txiao/shihan/compressed_pythia_data/pile_0.87_deduped_text_document"], "test_data_paths": ["/lustre/fast/fast/txiao/shihan/compressed_pythia_data/pile_0.87_deduped_text_document"], "valid_data_paths": ["/lustre/fast/fast/txiao/shihan/compressed_pythia_data/pile_0.87_deduped_text_document"], "train_data_weights": [1.0], "valid_data_weights": [1.0], "test_data_weights": [1.0], "data_impl": "mmap", "config_files": {"sepllm-160m-on-pythia-with-pile_deduped-n64H-NOkernel.yml": "{\n  # ##########Debug##########\n  #   \"world_size\": 1,\n  #   \"num_gpus\": 1,\n  # ########################\n\n  # parallelism settings\n  \"pipe-parallel-size\": 1,\n  \"model-parallel-size\": 1,\n\n  # model settings\n  \"num-layers\": 12,\n  \"hidden-size\": 768,\n  \"num-attention-heads\": 12,\n  \"seq-length\": 2048,\n  \"max-position-embeddings\": 2048,\n  \"pos-emb\": \"rotary\",\n  \"rotary-pct\": 0.25,\n  \"no-weight-tying\": true,\n  \"gpt-j-residual\": true,\n  \"output-layer-parallelism\": \"column\",\n  \"attention-config\": [[[\"global\"], 12]], \n\n  \"scaled_masked_softmax_fusion\": true, # For SepLLM\n  \"bias-gelu-fusion\": true,\n\n  # init methods\n  \"init_method\": \"small_init\",\n  \"output_layer_init_method\": \"wang_init\",\n\n  \"optimizer\": {\n    \"type\": \"Adam\",\n    \"params\": {\n      \"lr\": 0.0006,\n      \"betas\": [0.9, 0.95],\n      \"eps\": 1.0e-8\n    }\n  },\n  \"min_lr\": 0.00006,\n\n  \"zero_optimization\": {\n    \"stage\": 1,\n    \"allgather_partitions\": true,\n    \"allgather_bucket_size\": 500000000,\n    \"overlap_comm\": true,\n    \"reduce_scatter\": true,\n    \"reduce_bucket_size\": 500000000,\n    \"contiguous_gradients\": true,\n    \"cpu_offload\": false\n  },\n\n  # batch size (trained on 8 gpus)\n  \"train_micro_batch_size_per_gpu\": 32, # For 8 cards: if your GPU memory is not enough, you can set 64x2, 32x4, 16x8, etc., for 8 cards' machine, to keep the global train_batch_size as 1024.\n  \"gradient_accumulation_steps\": 4,  # For 8 cards: if your GPU memory is not enough, you can set 64x2, 32x4, 16x8, etc., for 8 cards' machine, to keep the global train_batch_size as 1024.\n  \"data-impl\": \"mmap\",\n  \"num_workers\": 1,\n\n  # activation checkpointing\n  \"checkpoint-activations\": true,\n  \"checkpoint-num-layers\": 1,\n  \"partition-activations\": true,\n  \"synchronize-each-layer\": true,\n\n  # regularization\n  \"gradient_clipping\": 1.0,\n  \"weight-decay\": 0.1,\n  \"hidden-dropout\": 0,\n  \"attention-dropout\": 0,\n\n  # precision settings\n  \"fp16\": {\n    \"fp16\": true,\n    \"enabled\": true,\n    \"loss_scale\": 0,\n    \"loss_scale_window\": 1000,\n    \"initial_scale_power\": 12,\n    \"hysteresis\": 2,\n    \"min_loss_scale\": 1\n  },\n\n  \"train-iters\": 143000,\n  \"lr-decay-iters\": 143000,\n  \"distributed-backend\": \"nccl\",\n  \"lr-decay-style\": \"cosine\",\n  \"warmup\": 0.01,\n  \"checkpoint-factor\": 1000,\n  \"extra-save-iters\": [0,1,2,4,8,16,32,64,128,256,512],\n  \"eval-interval\": 4000, \n  \"eval-iters\": 10,\n\n  \"log-interval\": 10,\n  \"steps_per_print\": 10,\n  \"wall_clock_breakdown\": true,\n\n  \"train-data-paths\": [\"/lustre/fast/fast/txiao/shihan/compressed_pythia_data/pile_0.87_deduped_text_document\"],\n  \"valid-data-paths\": [\"/lustre/fast/fast/txiao/shihan/compressed_pythia_data/pile_0.87_deduped_text_document\"],\n  \"test-data-paths\": [\"/lustre/fast/fast/txiao/shihan/compressed_pythia_data/pile_0.87_deduped_text_document\"],\n\n  \"tokenizer-type\": \"HFTokenizer\",\n  \"vocab-file\": \"/lustre/fast/fast/txiao/shihan_new/workspace/SepLLM/Training-SepLLM/sample_configs/20B_tokenizer.json\",\n\n  \"launcher\": \"pdsh\",\n\n  # \"save\": \"path/to/SepLLM-160m/checkpoints_n64h_8cards\",\n  # \"load\": \"path/to/SepLLM-160m/checkpoints_n64h_8cards\",\n  \n  ####################################################################### SepLLM #######################################################################:\n  # \"hostfile\": \"path/to/SepLLM/Training-SepLLM/sample_configs/hostfile\",\n\n  'separator_token_ids': [15, 13, 32, 2, 28, 27, 209, 186, 187], # For Pythia tokenizer  ## Fixed; The token ids for the special tokens (i.e. separators):  ['.', ',', '?', '!', ';', \":\", ' ', '\\t','\\n'].\n  'PADDING_ID': 0 , # For Pythia tokenizer  ## Fixed; The id for padding token of Pythia (GPT_NeoX)\n  \n\n  'USE_PREFILL_LOCAL_WIN_SIZES_wrt_LAYERS'  :   True,  # If True: the prefilling local window sizes for different self-attention layers are different; If True: should set 'prefill_loc_win_size_list', else: should set 'prefill_local_window_size'\n  'USE_GENERATE_LOCAL_WIN_SIZES_wrt_LAYERS' :   True,  # If True: the generating local window sizes for different self-attention layers are different; If True: should set 'generate_win_loc_size_list', else: should set 'generate_local_window_size'. USE_GENERATE_LOCAL_WIN_SIZES_wrt_LAYERS does not have any effect during the pretraining/prefilling phase.    \n\n  'prefill_loc_win_size_list' : [2048,   64,   64,   64,   64,   64,   64,   64,   64,   64, \n                                 64,   64],            ## The local window sizes for different self-attention layers when training (or prefilling). KVs for tokens inside the local window (we call them 'Neighboring Tokens') are kept and can been seen by the current token.                      \n\n  'generate_win_loc_size_list': [2048,   64,   64,   64,   64,   64,   64,   64,   64,   64, \n                                 64,   64],            ## The local window sizes for different self-attention layers when generating. KVs for tokens inside the local window (we call them 'Neighboring Tokens') are kept and can been seen by the current token. generate_win_loc_size_list does not have any effect during the pretraining/prefilling phase.\n\n  'init_tok_max_idx' :  2, # The largest index for the kept initial tokens. E.g., if init_tok_max_idx==2, it means we keep 3 initial tokens (idx: 0,1,2)\n  ######################################There should be at most 1 True for the following 3 args ##############################################\n  'USE_ORIGINAL_FULL_ATTEN' : False,  # Flag signal with the highest priority.  Train the Pythia model without any modification (standard full-attention version, i.e., standard upper triangular mask) if True.\n  'streamingLLM' :  False,  # Train streamingLLM. Only takes effect when USE_ORIGINAL_FULL_ATTEN=False. \n  'USE_SEP_ATTN_KERNEL_ACCELERATOR': False, # If True, use Sep_Attention module's kernel accelerator to accelerate the training process of SepLLM. If False (together with USE_ORIGINAL_FULL_ATTEN=False and streamingLLM=False), run plain SepLLM\n  ######################################There should be at most 1 True for the above 3 args ##############################################\n\n  'BATCH_ADAPTIVE_INIT_POS' : False,  # False by default.  If True: use the floating initial tokens' starting positions since when evaluating, LLM usually add paddings on the left side of the shorter seqs in a batch for alignment (i.e., left padding). Can be False when pretraining since the starting positions of initial tokens are at the beginning of each sequence in a batch for pretraining (i.e., right padding)\n  'PRINT_KV_RATIO' :  True,  # If True, print the KV cache preservation ratio (especially for the released trained model during generating). When pretraining, it will also print the retention ratio for the computational complexity of calculating the attention map if it is set True\n  'print_ratio_intervals': 8000,   # Print the retention ratio for the computational complexity of calculating the attention map once after every 'print_ratio_intervals' forward passes (or print_ratio_intervals/gradient_accumulation_steps  iterations). It only takes effect when PRINT_KV_RATIO=True.    \n  ####################################################################### ### #######################################################################\n}\n"}, "checkpoint_factor": 1000, "extra_save_iters": [0, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512], "batch_size": 32, "train_iters": 143000, "eval_iters": 10, "eval_interval": 4000, "vocab_file": "/lustre/fast/fast/txiao/shihan_new/workspace/SepLLM/Training-SepLLM/sample_configs/20B_tokenizer.json", "num_workers": 1, "checkpoint_activations": true, "synchronize_each_layer": true, "partition_activations": true, "dynamic_loss_scale": true, "pipe_parallel_size": 1, "world_size": 1, "is_pipe_parallel": true, "log_interval": 10, "text_gen_type": "unconditional", "local_rank": 0, "rank": 0, "user_script": "../../train.py", "save_iters": [0, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000, 11000, 12000, 13000, 14000, 15000, 16000, 17000, 18000, 19000, 20000, 21000, 22000, 23000, 24000, 25000, 26000, 27000, 28000, 29000, 30000, 31000, 32000, 33000, 34000, 35000, 36000, 37000, 38000, 39000, 40000, 41000, 42000, 43000, 44000, 45000, 46000, 47000, 48000, 49000, 50000, 51000, 52000, 53000, 54000, 55000, 56000, 57000, 58000, 59000, 60000, 61000, 62000, 63000, 64000, 65000, 66000, 67000, 68000, 69000, 70000, 71000, 72000, 73000, 74000, 75000, 76000, 77000, 78000, 79000, 80000, 81000, 82000, 83000, 84000, 85000, 86000, 87000, 88000, 89000, 90000, 91000, 92000, 93000, 94000, 95000, 96000, 97000, 98000, 99000, 100000, 101000, 102000, 103000, 104000, 105000, 106000, 107000, 108000, 109000, 110000, 111000, 112000, 113000, 114000, 115000, 116000, 117000, 118000, 119000, 120000, 121000, 122000, 123000, 124000, 125000, 126000, 127000, 128000, 129000, 130000, 131000, 132000, 133000, 134000, 135000, 136000, 137000, 138000, 139000, 140000, 141000, 142000], "global_num_gpus": 8, "separator_token_ids": [15, 13, 32, 2, 28, 27, 209, 186, 187], "USE_PREFILL_LOCAL_WIN_SIZES_wrt_LAYERS": true, "USE_GENERATE_LOCAL_WIN_SIZES_wrt_LAYERS": true, "prefill_loc_win_size_list": [2048, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64], "generate_win_loc_size_list": [2048, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64], "USE_SEP_ATTN_KERNEL_ACCELERATOR": false, "PRINT_KV_RATIO": true}
[2025-06-22 20:51:33,562] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-22 20:51:39,061] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2025-06-22 20:51:39,061] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=8, node_rank=0
[2025-06-22 20:51:39,061] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2025-06-22 20:51:39,062] [INFO] [launch.py:163:main] dist_world_size=8
[2025-06-22 20:51:39,062] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2025-06-22 20:51:41,775] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-22 20:51:41,836] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-22 20:51:41,859] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-22 20:51:42,002] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-22 20:51:42,012] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-22 20:51:42,013] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-22 20:51:42,031] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-22 20:51:42,055] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
fatal: not a git repository (or any parent up to mount point /lustre)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
fatal: not a git repository (or any parent up to mount point /lustre)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
############################################ Basic Args for SepAttention ############################################
self.USE_PREFILL_LOCAL_WIN_SIZES_wrt_LAYERS:  True
self.prefill_loc_win_size_list: [2048, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
self.USE_GENERATE_LOCAL_WIN_SIZES_wrt_LAYERS:  True
self.generate_win_loc_size_list: [2048, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
self.Layer_num: 12
self.init_tok_max_idx: 2
self.USE_ORIGINAL_FULL_ATTEN:  False
self.streamingLLM:  False
self.USE_SEP_ATTN_KERNEL_ACCELERATOR:  False
self.RECOMPILE_SEP_ATTN_KERNEL:  False
self.BATCH_ADAPTIVE_INIT_POS:  False
>>> For `BATCH_ADAPTIVE_INIT_POS`: Typically True when the input_ids of the model use 'left padding', False for 'right padding' (e.g., for training or some downstream tasks like `lambada_openai`, `piqa`, etc).
self.PRINT_KV_RATIO:  True
self.print_ratio_intervals:  8000
>>> Please be careful of the `separator_token_ids`, and make sure they are correct for the current LLM
self.separator_token_ids: [15, 13, 32, 2, 28, 27, 209, 186, 187]
self.USE_BiPE: False
self.EXCLUDE_DIAGONAL:  True
>>>>>>>>---------##########################################################-----------<<<<<<<<
>>>>>>>>---------                                                          -----------<<<<<<<<
>>>>>>>>--------------------- Running our SepLLM strategy ----------------------------<<<<<<<<
>>>>>>>>---------                                                          -----------<<<<<<<<
>>>>>>>>---------##########################################################-----------<<<<<<<<
Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
############################################ Basic Args for SepAttention ############################################
self.USE_PREFILL_LOCAL_WIN_SIZES_wrt_LAYERS:  True
self.prefill_loc_win_size_list: [2048, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
self.USE_GENERATE_LOCAL_WIN_SIZES_wrt_LAYERS:  True
self.generate_win_loc_size_list: [2048, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
self.Layer_num: 12
self.init_tok_max_idx: 2
self.USE_ORIGINAL_FULL_ATTEN:  False
self.streamingLLM:  False
self.USE_SEP_ATTN_KERNEL_ACCELERATOR:  False
self.RECOMPILE_SEP_ATTN_KERNEL:  False
self.BATCH_ADAPTIVE_INIT_POS:  False
>>> For `BATCH_ADAPTIVE_INIT_POS`: Typically True when the input_ids of the model use 'left padding', False for 'right padding' (e.g., for training or some downstream tasks like `lambada_openai`, `piqa`, etc).
self.PRINT_KV_RATIO:  True
self.print_ratio_intervals:  8000
>>> Please be careful of the `separator_token_ids`, and make sure they are correct for the current LLM
self.separator_token_ids: [15, 13, 32, 2, 28, 27, 209, 186, 187]
self.USE_BiPE: False
self.EXCLUDE_DIAGONAL:  True
>>>>>>>>---------##########################################################-----------<<<<<<<<
>>>>>>>>---------                                                          -----------<<<<<<<<
>>>>>>>>--------------------- Running our SepLLM strategy ----------------------------<<<<<<<<
>>>>>>>>---------                                                          -----------<<<<<<<<
>>>>>>>>---------##########################################################-----------<<<<<<<<
fatal: not a git repository (or any parent up to mount point /lustre)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
fatal: not a git repository (or any parent up to mount point /lustre)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
fatal: not a git repository (or any parent up to mount point /lustre)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
fatal: not a git repository (or any parent up to mount point /lustre)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
fatal: not a git repository (or any parent up to mount point /lustre)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
fatal: not a git repository (or any parent up to mount point /lustre)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
############################################ Basic Args for SepAttention ############################################
self.USE_PREFILL_LOCAL_WIN_SIZES_wrt_LAYERS:  True
self.prefill_loc_win_size_list: [2048, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
self.USE_GENERATE_LOCAL_WIN_SIZES_wrt_LAYERS:  True
self.generate_win_loc_size_list: [2048, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
self.Layer_num: 12
self.init_tok_max_idx: 2
self.USE_ORIGINAL_FULL_ATTEN:  False
self.streamingLLM:  False
self.USE_SEP_ATTN_KERNEL_ACCELERATOR:  False
self.RECOMPILE_SEP_ATTN_KERNEL:  False
self.BATCH_ADAPTIVE_INIT_POS:  False
>>> For `BATCH_ADAPTIVE_INIT_POS`: Typically True when the input_ids of the model use 'left padding', False for 'right padding' (e.g., for training or some downstream tasks like `lambada_openai`, `piqa`, etc).
self.PRINT_KV_RATIO:  True
self.print_ratio_intervals:  8000
>>> Please be careful of the `separator_token_ids`, and make sure they are correct for the current LLM
self.separator_token_ids: [15, 13, 32, 2, 28, 27, 209, 186, 187]
self.USE_BiPE: False
self.EXCLUDE_DIAGONAL:  True
>>>>>>>>---------##########################################################-----------<<<<<<<<
>>>>>>>>---------                                                          -----------<<<<<<<<
>>>>>>>>--------------------- Running our SepLLM strategy ----------------------------<<<<<<<<
>>>>>>>>---------                                                          -----------<<<<<<<<
>>>>>>>>---------##########################################################-----------<<<<<<<<
Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
############################################ Basic Args for SepAttention ############################################
self.USE_PREFILL_LOCAL_WIN_SIZES_wrt_LAYERS:  True
self.prefill_loc_win_size_list: [2048, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
self.USE_GENERATE_LOCAL_WIN_SIZES_wrt_LAYERS:  True
self.generate_win_loc_size_list: [2048, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
self.Layer_num: 12
self.init_tok_max_idx: 2
self.USE_ORIGINAL_FULL_ATTEN:  False
self.streamingLLM:  False
self.USE_SEP_ATTN_KERNEL_ACCELERATOR:  False
self.RECOMPILE_SEP_ATTN_KERNEL:  False
self.BATCH_ADAPTIVE_INIT_POS:  False
>>> For `BATCH_ADAPTIVE_INIT_POS`: Typically True when the input_ids of the model use 'left padding', False for 'right padding' (e.g., for training or some downstream tasks like `lambada_openai`, `piqa`, etc).
self.PRINT_KV_RATIO:  True
self.print_ratio_intervals:  8000
>>> Please be careful of the `separator_token_ids`, and make sure they are correct for the current LLM
self.separator_token_ids: [15, 13, 32, 2, 28, 27, 209, 186, 187]
self.USE_BiPE: False
self.EXCLUDE_DIAGONAL:  True
>>>>>>>>---------##########################################################-----------<<<<<<<<
>>>>>>>>---------                                                          -----------<<<<<<<<
>>>>>>>>--------------------- Running our SepLLM strategy ----------------------------<<<<<<<<
>>>>>>>>---------                                                          -----------<<<<<<<<
>>>>>>>>---------##########################################################-----------<<<<<<<<
Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
NeoXArgs.configure_distributed_args() using world size: 8 and model-parallel size: 1 
> building HFTokenizer tokenizer ...
Unable to import Mamba kernels. Install them from our requirements/requirements-mamba.txt,     or directly from https://github.com/state-spaces/mamba
############################################ Basic Args for SepAttention ############################################
self.USE_PREFILL_LOCAL_WIN_SIZES_wrt_LAYERS:  True
self.prefill_loc_win_size_list: [2048, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
self.USE_GENERATE_LOCAL_WIN_SIZES_wrt_LAYERS:  True
self.generate_win_loc_size_list: [2048, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
self.Layer_num: 12
self.init_tok_max_idx: 2
self.USE_ORIGINAL_FULL_ATTEN:  False
self.streamingLLM:  False
self.USE_SEP_ATTN_KERNEL_ACCELERATOR:  False
self.RECOMPILE_SEP_ATTN_KERNEL:  False
self.BATCH_ADAPTIVE_INIT_POS:  False
>>> For `BATCH_ADAPTIVE_INIT_POS`: Typically True when the input_ids of the model use 'left padding', False for 'right padding' (e.g., for training or some downstream tasks like `lambada_openai`, `piqa`, etc).
self.PRINT_KV_RATIO:  True
self.print_ratio_intervals:  8000
>>> Please be careful of the `separator_token_ids`, and make sure they are correct for the current LLM
self.separator_token_ids: [15, 13, 32, 2, 28, 27, 209, 186, 187]
self.USE_BiPE: False
self.EXCLUDE_DIAGONAL:  True
>>>>>>>>---------##########################################################-----------<<<<<<<<
>>>>>>>>---------                                                          -----------<<<<<<<<
>>>>>>>>--------------------- Running our SepLLM strategy ----------------------------<<<<<<<<
>>>>>>>>---------                                                          -----------<<<<<<<<
>>>>>>>>---------##########################################################-----------<<<<<<<<
############################################ Basic Args for SepAttention ############################################
self.USE_PREFILL_LOCAL_WIN_SIZES_wrt_LAYERS:  True
self.prefill_loc_win_size_list: [2048, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
self.USE_GENERATE_LOCAL_WIN_SIZES_wrt_LAYERS:  True
self.generate_win_loc_size_list: [2048, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
self.Layer_num: 12
self.init_tok_max_idx: 2
self.USE_ORIGINAL_FULL_ATTEN:  False
self.streamingLLM:  False
self.USE_SEP_ATTN_KERNEL_ACCELERATOR:  False
self.RECOMPILE_SEP_ATTN_KERNEL:  False
self.BATCH_ADAPTIVE_INIT_POS:  False
>>> For `BATCH_ADAPTIVE_INIT_POS`: Typically True when the input_ids of the model use 'left padding', False for 'right padding' (e.g., for training or some downstream tasks like `lambada_openai`, `piqa`, etc).
self.PRINT_KV_RATIO:  True
self.print_ratio_intervals:  8000
>>> Please be careful of the `separator_token_ids`, and make sure they are correct for the current LLM
self.separator_token_ids: [15, 13, 32, 2, 28, 27, 209, 186, 187]
self.USE_BiPE: False
self.EXCLUDE_DIAGONAL:  True
>>>>>>>>---------##########################################################-----------<<<<<<<<
>>>>>>>>---------                                                          -----------<<<<<<<<
>>>>>>>>--------------------- Running our SepLLM strategy ----------------------------<<<<<<<<
>>>>>>>>---------                                                          -----------<<<<<<<<
>>>>>>>>---------##########################################################-----------<<<<<<<<
 > padded vocab (size: 50277) with 27 dummy tokens (new size: 50304)
############################################ Basic Args for SepAttention ############################################
self.USE_PREFILL_LOCAL_WIN_SIZES_wrt_LAYERS:  True
self.prefill_loc_win_size_list: [2048, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
self.USE_GENERATE_LOCAL_WIN_SIZES_wrt_LAYERS:  True
self.generate_win_loc_size_list: [2048, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
self.Layer_num: 12
self.init_tok_max_idx: 2
self.USE_ORIGINAL_FULL_ATTEN:  False
self.streamingLLM:  False
self.USE_SEP_ATTN_KERNEL_ACCELERATOR:  False
self.RECOMPILE_SEP_ATTN_KERNEL:  False
self.BATCH_ADAPTIVE_INIT_POS:  False
>>> For `BATCH_ADAPTIVE_INIT_POS`: Typically True when the input_ids of the model use 'left padding', False for 'right padding' (e.g., for training or some downstream tasks like `lambada_openai`, `piqa`, etc).
self.PRINT_KV_RATIO:  True
self.print_ratio_intervals:  8000
>>> Please be careful of the `separator_token_ids`, and make sure they are correct for the current LLM
self.separator_token_ids: [15, 13, 32, 2, 28, 27, 209, 186, 187]
self.USE_BiPE: False
self.EXCLUDE_DIAGONAL:  True
>>>>>>>>---------##########################################################-----------<<<<<<<<
>>>>>>>>---------                                                          -----------<<<<<<<<
>>>>>>>>--------------------- Running our SepLLM strategy ----------------------------<<<<<<<<
>>>>>>>>---------                                                          -----------<<<<<<<<
>>>>>>>>---------##########################################################-----------<<<<<<<<
############################################ Basic Args for SepAttention ############################################
self.USE_PREFILL_LOCAL_WIN_SIZES_wrt_LAYERS:  True
self.prefill_loc_win_size_list: [2048, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
self.USE_GENERATE_LOCAL_WIN_SIZES_wrt_LAYERS:  True
self.generate_win_loc_size_list: [2048, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]
self.Layer_num: 12
self.init_tok_max_idx: 2
self.USE_ORIGINAL_FULL_ATTEN:  False
self.streamingLLM:  False
self.USE_SEP_ATTN_KERNEL_ACCELERATOR:  False
self.RECOMPILE_SEP_ATTN_KERNEL:  False
self.BATCH_ADAPTIVE_INIT_POS:  False
>>> For `BATCH_ADAPTIVE_INIT_POS`: Typically True when the input_ids of the model use 'left padding', False for 'right padding' (e.g., for training or some downstream tasks like `lambada_openai`, `piqa`, etc).
self.PRINT_KV_RATIO:  True
self.print_ratio_intervals:  8000
>>> Please be careful of the `separator_token_ids`, and make sure they are correct for the current LLM
self.separator_token_ids: [15, 13, 32, 2, 28, 27, 209, 186, 187]
self.USE_BiPE: False
self.EXCLUDE_DIAGONAL:  True
>>>>>>>>---------##########################################################-----------<<<<<<<<
>>>>>>>>---------                                                          -----------<<<<<<<<
>>>>>>>>--------------------- Running our SepLLM strategy ----------------------------<<<<<<<<
>>>>>>>>---------                                                          -----------<<<<<<<<
>>>>>>>>---------##########################################################-----------<<<<<<<<

